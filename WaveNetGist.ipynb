{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from tensorflow import keras\n",
    "from keras import layers\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n"
     ]
    }
   ],
   "source": [
    "os.environ['TFF_CPP_MIN_LOG_LEVEL'] = '0'\n",
    "physical_devices = tf.config.list_physical_devices(\"GPU\")\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "print(physical_devices)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "with open(f'data/x_train_sample.npy', 'rb') as f:\n",
    "    X_train = np.load(f, allow_pickle=True)\n",
    "with open(f'data/y_train_sample.npy', 'rb') as f:\n",
    "    Y_train = np.load(f, allow_pickle=True)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "#initial hyper-parameters\n",
    "output_classes = 120\n",
    "seq_len = 50\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], X_train.shape[1], 1).astype(\"float32\") / (output_classes-1)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "class DilatedCausalConvolution1D(layers.Layer):\n",
    "    def __init__(self, filters, kernel_size, dilation_rate):\n",
    "        super(DilatedCausalConvolution1D, self).__init__()\n",
    "        self.conv1D = keras.layers.Conv1D(filters=filters, kernel_size=kernel_size,\n",
    "                                          dilation_rate=dilation_rate,\n",
    "                                          padding=\"causal\")\n",
    "        self.index_of_ignored_data = (kernel_size - 1) * dilation_rate\n",
    "\n",
    "    def call(self, x, training=False):\n",
    "        x = self.conv1D(x, training=training)\n",
    "        return x[:, self.index_of_ignored_data:]\n",
    "\n",
    "\n",
    "class ResidualBlock(layers.Layer):\n",
    "    def __init__(self, res_channels, skip_channels, kernel_size, dilation_rate):\n",
    "        super(ResidualBlock, self).__init__()\n",
    "        self.dilated_causal_convolution = DilatedCausalConvolution1D(res_channels, kernel_size,\n",
    "                                                                     dilation_rate=dilation_rate)\n",
    "        self.residual_conv1D = layers.Conv1D(filters=res_channels, kernel_size=1)\n",
    "        self.skip_conv1D = layers.Conv1D(filters=skip_channels, kernel_size=1)\n",
    "\n",
    "    def call(self, input_x, skip_size, training=False):\n",
    "        x = self.dilated_causal_convolution(input_x, training=training)\n",
    "        tanh_result = keras.activations.tanh(x)\n",
    "        sigmoid_result = keras.activations.sigmoid(x)\n",
    "        x = tanh_result * sigmoid_result\n",
    "        residual_output = self.residual_conv1D(x)\n",
    "        residual_output = residual_output + input_x[:, -residual_output.shape[1]:]\n",
    "        skip_connection_output = self.skip_conv1D(x)\n",
    "        skip_connection_output = skip_connection_output[:, -skip_size:]\n",
    "        return residual_output, skip_connection_output\n",
    "\n",
    "\n",
    "class StacksResidualBlocks(layers.Layer):\n",
    "    def __init__(self, residual_channels, skip_channels, kernel_size, stack_size, layer_size):\n",
    "        super(StacksResidualBlocks, self).__init__()\n",
    "        build_dilation_function = self.build_dilation\n",
    "        dilations = build_dilation_function(stack_size, layer_size)\n",
    "        self.residual_blocks = []\n",
    "\n",
    "        for stack_level, dilations_per_stack in enumerate(dilations):\n",
    "            for layer_level, dilation_rate in enumerate(dilations_per_stack):\n",
    "                residual_block = ResidualBlock(residual_channels, skip_channels, kernel_size, dilation_rate)\n",
    "                self.residual_blocks.append(residual_block)\n",
    "\n",
    "    def build_dilation(self, stack_size, layer_size):\n",
    "        stacks_dilations = []\n",
    "        for stack in range(stack_size):\n",
    "            dilations = []\n",
    "            for layer in range(layer_size):\n",
    "                dilations.append(2 ** layer)\n",
    "            stacks_dilations.append(dilations)\n",
    "        return stacks_dilations\n",
    "\n",
    "    def call(self, x, skip_size, training=False):\n",
    "        residual_output = x\n",
    "        skip_connection_outputs = []\n",
    "        for residual_block in self.residual_blocks:\n",
    "            residual_output, skip_connection_output = residual_block(residual_output, skip_size)\n",
    "            skip_connection_outputs.append(skip_connection_output)\n",
    "        return residual_output, tf.convert_to_tensor(skip_connection_outputs)\n",
    "\n",
    "\n",
    "class DenseLayer(layers.Layer):\n",
    "    def __init__(self, channel, num_classes):\n",
    "        super(DenseLayer, self).__init__()\n",
    "        self.conv1D = layers.Conv1D(filters=channel, kernel_size=1)\n",
    "        self.dense = layers.Dense(num_classes)\n",
    "        self.flatten = layers.Flatten()\n",
    "\n",
    "    def call(self, skip_connection_outputs, training=False):\n",
    "        x = tf.reduce_mean(skip_connection_outputs, 0)\n",
    "        x = keras.activations.relu(x)\n",
    "        x = self.conv1D(x, training=training)\n",
    "        x = keras.activations.relu(x)\n",
    "        x = self.conv1D(x, training=training)\n",
    "        x = self.flatten(x)\n",
    "        x = self.dense(x)\n",
    "        x = keras.activations.softmax(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class WaveNet(keras.Model):\n",
    "    def __init__(self, input_channels, output_channels, kernel_size, stack_size, layer_size, num_classes):\n",
    "        super(WaveNet, self).__init__()\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stack_size = stack_size\n",
    "        self.layer_size = layer_size\n",
    "        self.causal_conv1D = DilatedCausalConvolution1D(output_channels, kernel_size, dilation_rate=1)\n",
    "        self.stack_residual_blocks = StacksResidualBlocks(input_channels, output_channels, kernel_size, stack_size,\n",
    "                                                          layer_size)\n",
    "        self.classifier = DenseLayer(output_channels, num_classes)\n",
    "\n",
    "    def calculate_receptive_field(self):\n",
    "        return np.sum([(self.kernel_size - 1) * (2 ** level) for level in range(self.layer_size)] * self.stack_size)\n",
    "\n",
    "    def calculate_skip_size(self, x):\n",
    "        return x.shape[1] - self.calculate_receptive_field()\n",
    "\n",
    "    def call(self, input_x, training=False):\n",
    "        x = self.causal_conv1D(input_x)\n",
    "        skip_size = self.calculate_skip_size(x)\n",
    "        _, skip_connection_outputs = self.stack_residual_blocks(input_x, skip_size=skip_size, training=training)\n",
    "        x = self.classifier(skip_connection_outputs)\n",
    "        return x\n",
    "\n",
    "    def model(self):\n",
    "        x = keras.Input(shape=(seq_len, 1))\n",
    "        return keras.Model(inputs=[x], outputs=self.call(x))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "#additional hyper-parameters\n",
    "residual_channel = 128\n",
    "skip_channel = 256\n",
    "kernel_size = 2\n",
    "stack_size = 2\n",
    "layer_size = 3\n",
    "\n",
    "model = WaveNet(residual_channel, skip_channel, kernel_size, stack_size, layer_size, output_classes)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# assigning training hyper-parameters\n",
    "model.compile(\n",
    "    optimizer=keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss=tf.keras.losses.SparseCategoricalCrossentropy(from_logits=False),\n",
    "    metrics=[\"accuracy\"]\n",
    ")\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 50, 1)]           0         \n",
      "                                                                 \n",
      " stacks_residual_blocks (Sta  ((None, 36, 128),        462080    \n",
      " cksResidualBlocks)           (6, None, 35, 256))                \n",
      "                                                                 \n",
      " dense_layer (DenseLayer)    (None, 120)               1141112   \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 1,603,192\n",
      "Trainable params: 1,603,192\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(model.model().summary())\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dilated_causal_convolution1d/conv1d/kernel:0', 'dilated_causal_convolution1d/conv1d/bias:0', 'stacks_residual_blocks/residual_block_5/conv1d_17/kernel:0', 'stacks_residual_blocks/residual_block_5/conv1d_17/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "WARNING:tensorflow:Gradients do not exist for variables ['dilated_causal_convolution1d/conv1d/kernel:0', 'dilated_causal_convolution1d/conv1d/bias:0', 'stacks_residual_blocks/residual_block_5/conv1d_17/kernel:0', 'stacks_residual_blocks/residual_block_5/conv1d_17/bias:0'] when minimizing the loss. If you're using `model.compile()`, did you forget to provide a `loss`argument?\n",
      "201/201 - 13s - loss: 3.4710 - accuracy: 0.0963 - 13s/epoch - 65ms/step\n",
      "Epoch 2/20\n",
      "201/201 - 5s - loss: 2.8423 - accuracy: 0.2051 - 5s/epoch - 27ms/step\n",
      "Epoch 3/20\n",
      "201/201 - 5s - loss: 2.2622 - accuracy: 0.3601 - 5s/epoch - 26ms/step\n",
      "Epoch 4/20\n",
      "201/201 - 5s - loss: 1.6642 - accuracy: 0.5323 - 5s/epoch - 27ms/step\n",
      "Epoch 5/20\n",
      "201/201 - 5s - loss: 1.1989 - accuracy: 0.6553 - 5s/epoch - 27ms/step\n",
      "Epoch 6/20\n",
      "201/201 - 5s - loss: 0.8430 - accuracy: 0.7601 - 5s/epoch - 26ms/step\n",
      "Epoch 7/20\n",
      "201/201 - 5s - loss: 0.6282 - accuracy: 0.8223 - 5s/epoch - 27ms/step\n",
      "Epoch 8/20\n",
      "201/201 - 6s - loss: 0.4595 - accuracy: 0.8770 - 6s/epoch - 28ms/step\n",
      "Epoch 9/20\n",
      "201/201 - 5s - loss: 0.3906 - accuracy: 0.8954 - 5s/epoch - 27ms/step\n",
      "Epoch 10/20\n",
      "201/201 - 5s - loss: 0.3245 - accuracy: 0.9203 - 5s/epoch - 27ms/step\n",
      "Epoch 11/20\n",
      "201/201 - 5s - loss: 0.2863 - accuracy: 0.9345 - 5s/epoch - 27ms/step\n",
      "Epoch 12/20\n",
      "201/201 - 5s - loss: 0.2478 - accuracy: 0.9434 - 5s/epoch - 27ms/step\n",
      "Epoch 13/20\n",
      "201/201 - 5s - loss: 0.2178 - accuracy: 0.9554 - 5s/epoch - 27ms/step\n",
      "Epoch 14/20\n",
      "201/201 - 6s - loss: 0.1960 - accuracy: 0.9604 - 6s/epoch - 28ms/step\n",
      "Epoch 15/20\n",
      "201/201 - 6s - loss: 0.1969 - accuracy: 0.9618 - 6s/epoch - 29ms/step\n",
      "Epoch 16/20\n",
      "201/201 - 5s - loss: 0.1994 - accuracy: 0.9598 - 5s/epoch - 27ms/step\n",
      "Epoch 17/20\n",
      "201/201 - 5s - loss: 0.1811 - accuracy: 0.9598 - 5s/epoch - 27ms/step\n",
      "Epoch 18/20\n",
      "201/201 - 5s - loss: 0.1738 - accuracy: 0.9665 - 5s/epoch - 27ms/step\n",
      "Epoch 19/20\n",
      "201/201 - 5s - loss: 0.1618 - accuracy: 0.9694 - 5s/epoch - 27ms/step\n",
      "Epoch 20/20\n",
      "201/201 - 7s - loss: 0.1758 - accuracy: 0.9713 - 7s/epoch - 33ms/step\n"
     ]
    }
   ],
   "source": [
    "history = model.fit(X_train, Y_train, batch_size=32, epochs=20, verbose=2)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}